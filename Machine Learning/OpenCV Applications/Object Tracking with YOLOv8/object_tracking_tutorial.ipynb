{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Vision Object Tracking\n",
    "[Link to Tutorial](https://medium.com/mjrobot-org/automatic-vision-object-tracking-2dc6b4acaff5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specs: YOLOv8 Nano, image size = 640, epochs = 5, batch size = 3, cpu-only\n",
    "\n",
    "### Arguments Explained\n",
    "- imgsz = 640\n",
    "    - Defines the size of the input images (in pixels) that the model will process during training.\n",
    "\n",
    "    - A value of 1280 means that images will be resized to 1280x1280 pixels. Larger image sizes typically lead to better accuracy but increase computational cost.\n",
    "\n",
    "- epochs = 5\n",
    "    - Specifies the number of training iterations (epochs).\n",
    "\n",
    "    - An epoch means the model goes through the entire training dataset once.\n",
    "\n",
    "- batch = 3:\n",
    "\n",
    "    - Sets the batch size, meaning the model will process 3 images at a time during training.\n",
    "\n",
    "    - Batch size impacts memory usage (GPU/CPU) and training speed:\n",
    "        - Smaller batch size: Lower memory requirement but slower convergence.\n",
    "        - Larger batch size: Faster convergence but higher memory requirement.\n",
    "\n",
    "- device = 'cpu'\n",
    "    - gpu or tpu could be enabled, but my local set-up did not have CUDA enabled (I have an AMD Graphics Card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Load the model\n",
    "model = YOLO('yolov8n.pt')\n",
    " \n",
    "# Training.\n",
    "results = model.train(\n",
    "   data='C:/Users/fycce/Documents/GitHub/Online_tutorials/Machine Learning/OpenCV Applications/Object Tracking/Rock Paper Scissors SXSW.v14i.yolov8/data.yaml',\n",
    "   imgsz=640,\n",
    "   epochs=5,\n",
    "   batch=3,\n",
    "   name='yolov8n_v8_50e',\n",
    "   device='cpu'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Training the neuronal network is very cpu-intensive given that I only used my PC's CPU. Hence, I had to limit the specs of the model training given that a regular epoch already took me 21-24 minutes to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During Training?\n",
    "\n",
    "- The dataset is loaded and split into batches (size = 3).\n",
    "- Each image is resized to 640 x 640 pixels.\n",
    "- The model fine-tunes its weights over 5 epochs:\n",
    "    - Updates weights to minimize loss using gradient descent.\n",
    "    - Computes validation metrics after each epoch to monitor performance.\n",
    "- At the end of training, the model saves the best weights based on validation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the actual output of the script after the neuronal network was done training.\n",
    "At best, no image resizing at all, slightly larger batch sizes and all of the same previous settings should have made the epochs shorter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ultralytics 8.3.38  Python-3.12.3 torch-2.5.1 CPU (AMD Ryzen 7 5800H with Radeon Graphics)\n",
    "engine\\trainer: task=detect, mode=train, model=yolov8n.pt, data=C:/Users/fycce/Documents/GitHub/Online_tutorials/Machine Learning/OpenCV Applications/Object Tracking/Rock Paper Scissors SXSW.v14i.yolov8/data.yaml, epochs=5, time=None, patience=100, batch=3, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=yolov8n_v8_50e, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\yolov8n_v8_50e\n",
    "Overriding model.yaml nc=80 with nc=3\n",
    "\n",
    "                   from  n    params  module                                       arguments                     \n",
    "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
    "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
    "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
    "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
    "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
    "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
    "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
    "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
    "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
    "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
    " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
    " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
    " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
    " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
    " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
    "...\n",
    "\n",
    "Transferred 319/355 items from pretrained weights\n",
    "TensorBoard: Start with 'tensorboard --logdir runs\\detect\\yolov8n_v8_50e', view at http://localhost:6006/\n",
    "Freezing layer 'model.22.dfl.conv.weight'\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "train: Scanning C:\\Users\\fycce\\Documents\\GitHub\\Online_tutorials\\Machine Learning\\OpenCV Applications\\Object Tracking\\Rock Paper Scissors SXSW.v14i.yolov8\\train\\labels.cache... 6455 images, 2516 backgrounds, 0 corrupt: 100%|██████████| 6455/6455 [00:00<?, ?it/s]\n",
    "val: Scanning C:\\Users\\fycce\\Documents\\GitHub\\Online_tutorials\\Machine Learning\\OpenCV Applications\\Object Tracking\\Rock Paper Scissors SXSW.v14i.yolov8\\valid\\labels.cache... 576 images, 238 backgrounds, 0 corrupt: 100%|██████████| 576/576 [00:00<?, ?it/s]\n",
    "Plotting labels to runs\\detect\\yolov8n_v8_50e\\labels.jpg... \n",
    "optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
    "optimizer: AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0004921875), 63 bias(decay=0.0)\n",
    "TensorBoard: model graph visualization added \n",
    "Image sizes 640 train, 640 val\n",
    "Using 0 dataloader workers\n",
    "Logging results to runs\\detect\\yolov8n_v8_50e\n",
    "Starting training for 5 epochs...\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        1/5         0G        1.4      4.026      1.569          3        640: 100%|██████████| 2152/2152 [22:04<00:00,  1.63it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:46<00:00,  2.08it/s]\n",
    "                   all        576        400      0.428        0.4      0.384      0.233\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        2/5         0G      1.408      3.002      1.566          2        640: 100%|██████████| 2152/2152 [21:00<00:00,  1.71it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:41<00:00,  2.31it/s]\n",
    "                   all        576        400      0.404      0.572      0.491      0.272\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        3/5         0G       1.38      2.394      1.535          0        640: 100%|██████████| 2152/2152 [20:03<00:00,  1.79it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:41<00:00,  2.34it/s]\n",
    "                   all        576        400      0.683      0.477      0.608       0.38\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        4/5         0G      1.286      1.995      1.473          3        640: 100%|██████████| 2152/2152 [20:20<00:00,  1.76it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:40<00:00,  2.36it/s]\n",
    "                   all        576        400      0.696      0.731      0.764      0.507\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "        5/5         0G      1.173      1.692      1.389          6        640: 100%|██████████| 2152/2152 [19:41<00:00,  1.82it/s]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:40<00:00,  2.38it/s]\n",
    "                   all        576        400       0.84      0.752      0.842      0.599\n",
    "\n",
    "\n",
    "5 epochs completed in 1.779 hours.\n",
    "Optimizer stripped from runs\\detect\\yolov8n_v8_50e\\weights\\last.pt, 6.2MB\n",
    "Optimizer stripped from runs\\detect\\yolov8n_v8_50e\\weights\\best.pt, 6.2MB\n",
    "\n",
    "Validating runs\\detect\\yolov8n_v8_50e\\weights\\best.pt...\n",
    "Ultralytics 8.3.38  Python-3.12.3 torch-2.5.1 CPU (AMD Ryzen 7 5800H with Radeon Graphics)\n",
    "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 96/96 [00:34<00:00,  2.77it/s]\n",
    "                   all        576        400      0.841      0.752      0.842      0.599\n",
    "                 Paper        132        139      0.799      0.662       0.78      0.517\n",
    "                  Rock        121        141      0.859      0.851      0.884      0.656\n",
    "              Scissors        116        120      0.864      0.742      0.863      0.623\n",
    "Speed: 1.6ms preprocess, 52.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
    "Results saved to runs\\detect\\yolov8n_v8_50e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model on Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Rock, 46.0ms\n",
      "Speed: 2.1ms preprocess, 46.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Rock, 52.6ms\n",
      "Speed: 0.5ms preprocess, 52.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Rock, 43.9ms\n",
      "Speed: 1.0ms preprocess, 43.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 Rocks, 43.1ms\n",
      "Speed: 1.0ms preprocess, 43.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Paper, 39.8ms\n",
      "Speed: 1.0ms preprocess, 39.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 Paper, 42.4ms\n",
      "Speed: 1.5ms preprocess, 42.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\fycce\\Documents\\GitHub\\Online_tutorials\\Machine Learning\\OpenCV Applications\\Object Tracking with YOLOv8\\runs\\detect\\yolov8n_v8_50e\\weights')\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# laptop/pc main camera\n",
    "capt = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capt.read()\n",
    "    result = model.predict(frame)\n",
    "\n",
    "    boxes = result[0].boxes\n",
    "    for box in boxes:\n",
    "        # get xyxy coordinates from 1st box\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "\n",
    "        confidence = box.conf[0].item()\n",
    "        \n",
    "        # get prediction from 1st box\n",
    "        class_id = int(box.cls[0].item())\n",
    "        class_name = model.names[class_id]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        label = f\"{class_name} {confidence:.2f}\"\n",
    "\n",
    "        cv2.putText(frame, label,\n",
    "                     (x1-10, y1-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                      0.7,\n",
    "                      (0, 255, 255), 2)\n",
    "        # necessary due to several box objects in boxes\n",
    "        continue\n",
    "        \n",
    "    cv2.imshow(\"YOLOv8 Webcam Predictions\", frame)\n",
    "\n",
    "    # press esc key to exit\n",
    "    k = cv2.waitKey(1000)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "capt.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the model predicts accurately enough given that minimal image resizing was used and the model was trained over only 5 epochs.\n",
    "Rock had the highest preidiction accuracy for me (surely because it is starkly different from paper and scissors). \n",
    "\n",
    "### More conclusions will be made soon based on the graphs provided by YOLOv8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs After Training\n",
    "1. Trained Model Weights:\n",
    "    - fine-tuned weights file is saved in the runs/detect/... directory\n",
    "    - it contains **best.pt** (model with the best validation performance) and **last.pt** (final model after all epochs)\n",
    "\n",
    "2. Training Logs:\n",
    "    - Training metrics such as loss, precision, recall, mAP (mean Average Precision), etc., will be logged.\n",
    "    - A results.csv file may also be generated with detailed metrics.\n",
    "\n",
    "3. Visualized Metrics:\n",
    "    - Training and validation curves (e.g., loss, mAP) will be saved as .png files in the same directory runs/detect/...\n",
    "\n",
    "4. Evaluation Results:\n",
    "    - After training, the model's performance is evaluated on the validation dataset. Metrics include:\n",
    "        - mAP50: Mean Average Precision at IoU=0.5.\n",
    "        - mAP50:95: mAP averaged across IoUs from 0.5 to 0.95.\n",
    "        - Precision and Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use Model version 14 on the website through the API:\n",
    "[Rock-Paper-Scissors Annotated Dataset](https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw/dataset/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "\n",
    "capt = cv2.VideoCapture(0)\n",
    "rf = Roboflow(api_key=\"insert-your-own-key\")\n",
    "project = rf.workspace().project(\"rock-paper-scissors-sxsw\")\n",
    "model = project.version(14).model\n",
    "\n",
    "while True:\n",
    "    ret, frame = capt.read()\n",
    "    result = model.predict(frame).json()\n",
    "    predictions = result[\"predictions\"]\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        x = prediction[\"x\"]  # center x coordinate\n",
    "        y = prediction[\"y\"]  # center y coordinate\n",
    "        width = prediction[\"width\"]\n",
    "        height = prediction[\"height\"]\n",
    "        confidence = prediction[\"confidence\"]\n",
    "        clas = prediction[\"class\"]\n",
    "    \n",
    "    if (predictions != []):\n",
    "        cv2.rectangle(frame, (int(x - width/2), int(y - height/2)), (int(x + width/2), int(y + height/2)), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (int(x - 10), int(y - 10)), (int(x + 10), int(y + 10)), (0, 255, 0), 2)\n",
    "    \n",
    "        label = f\"{clas} {confidence:.2f}\"\n",
    "\n",
    "        cv2.putText(frame,\n",
    "                  label,\n",
    "                      (int(x), int(y - height/2 - 10)), \n",
    "                     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                       0.7,\n",
    "                       (0, 255, 0), 2)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"YOLOv8 Webcam Predictions\", frame)\n",
    "\n",
    "    # press esc key to exit\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "capt.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The Roboflow pre-trained model was far more accurate than mine, which is understandable given the very low specs of mine. Rock can be distinguished very easily compared to the others and has often a high level of accuracy. The images below give examples of the predictions made by my model and the other pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MyModel:Rock](./image_results/rock1.png)\n",
    "![MyModel:Scissors](./image_results/scissors1.png)\n",
    "![MyModel:Paper](./image_results/paper1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roboflow Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Roboflow:Rock](./image_results/rock2.png)\n",
    "![Roboflow:Scissors](./image_results/scissors2.png)\n",
    "![Roboflow:Paper](./image_results/paper2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trolling the AI (my model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dud:Rock](./image_results/dud_rock.png)\n",
    "![Dud:Scissors](./image_results/dud_scissors.png)\n",
    "![Dud:Paper](./image_results/dud_paper.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_vision_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
